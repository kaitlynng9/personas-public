{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import ast\n",
    "# from sodapy import Socrata\n",
    "from collections import Counter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a csv file\n",
    "person = # download PSRC HTS person-level data\n",
    "trips = # download PSRC HTS trip-level data\n",
    "household = # download PSRC HTS household-level data\n",
    "od = # insert dataset with associated origin-destiation lat/lng for household travel survey\n",
    "home_loc = # insert dataset with associated home location lat/lng for household travel survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join and clean data\n",
    "- Identify which individuals have made restaurant trips\n",
    "- gather ALL the trip data these individuals have made\n",
    "- join O/D information to each\n",
    "\n",
    "Inclduing weekend trips, we have 20,186 total restaurent trips made by 4045 individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only restaurant trips\n",
    "restaurant_trips = trips[(trips[\"origin_purpose\"]==\"Went to restaurant to eat/get take-out\")\n",
    "                            | (trips[\"dest_purpose\"]==\"Went to restaurant to eat/get take-out\")]\n",
    "\n",
    "# individuals who visited restaurants\n",
    "rest_person_ids = restaurant_trips[\"person_dim_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove na values\n",
    "trips_clean = trips[(trips.arrival_time_string.notna()) & trips.depart_time_string.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all trip data for a person if they made a restaurant trip\n",
    "selected_trips = trips_clean[trips_clean.person_dim_id.isin(rest_person_ids)]\n",
    "selected_trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to manually calculate travel time so we don't have to drop rows\n",
    "def calc_travel(row):\n",
    "    date_str_arr = row.arrival_time_string[:-1]\n",
    "    date_str_dep = row.depart_time_string[:-1]\n",
    "    date_format = \"Date: %Y-%m-%d %H:%M:%S.%f\"\n",
    "\n",
    "    # convert string to datetime object\n",
    "    dt_object_arr = datetime.strptime(date_str_arr, date_format)\n",
    "    dt_object_dep = datetime.strptime(date_str_dep, date_format)\n",
    "    \n",
    "    # calculate travel time\n",
    "    delta_time = dt_object_arr - dt_object_dep\n",
    "    travel_time_mins = delta_time.total_seconds()/60\n",
    "\n",
    "    return travel_time_mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-calculate column for trip time\n",
    "selected_trips[\"travel_time_calc\"] = selected_trips.apply(calc_travel, axis=1)\n",
    "\n",
    "# reduce dataset to only include trips with travel time < 300\n",
    "selected_trips_clean = selected_trips[selected_trips.travel_time_calc < 120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join o/d information\n",
    "trips_od = pd.merge(selected_trips_clean, od, on=[\"trip_id\", \"household_id\"], how=\"left\")\n",
    "# drop unecessary columns\n",
    "trips_od = trips_od.drop([\"person_id\", \"ObjectId\"], axis = 1)\n",
    "# drop 2021\n",
    "trips_od = trips_od[trips_od[\"survey_year\"] != 2021]\n",
    "# trips_od = trips_od[(trips_od[\"daynum\"] != 6) | (trips_od[\"daynum\"] != 7)]\n",
    "trips_od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_od_rest = trips_od[(trips_od[\"origin_purpose\"]==\"Went to restaurant to eat/get take-out\")\n",
    "                            | (trips_od[\"dest_purpose\"]==\"Went to restaurant to eat/get take-out\")]\n",
    "trips_od_rest.to_csv('data_outputs/trips_od_rest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each person, obtain their most frequent day num\n",
    "max_day_per_person = {}\n",
    "for person in trips_od_rest.person_dim_id.unique():\n",
    "    person_trips = trips_od_rest[trips_od_rest.person_dim_id == person]\n",
    "    max_day_per_person[person] = person_trips['daynum'].value_counts().idxmax()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what % of each class is a frequent weekender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = pd.read_table(\"mplus_analysis/4_class_probabilities.dat\", sep=\"\\s+\", header=None)\n",
    "person_data = pd.read_csv(\"mplus_analysis/lca_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claen data\n",
    "person_prob = pd.concat([probabilities[34], person_data], axis=1)\n",
    "person_prob = person_prob.rename(columns={34:\"Class\"})\n",
    "# person_prob = person_prob.drop(\"Unnamed: 0\", axis=1)\n",
    "person_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of people with each maxdaynum in percentage\n",
    "max_day_counts = Counter(max_day_per_person.values())\n",
    "max_day_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df of person based on persion_id, class, and max day num\n",
    "person_class = person_prob[[\"person_id\", \"Class\"]].copy(deep=True)\n",
    "\n",
    "# add col based on max day num\n",
    "person_class[\"max_day\"] = person_class[\"person_id\"].map(max_day_per_person)\n",
    "person_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each class, whawt % is a weekender? weekend daynums = 5,6\n",
    "print(\"Class 1:\" )\n",
    "# print(person_class[person_class.Class == 1].value_counts(\"max_day\", normalize=True).sort_index(ascending=True))\n",
    "# print sum of 5 & 6, percentage\n",
    "print(person_class[person_class.Class == 1].value_counts(\"max_day\", normalize=True).sort_index(ascending=True)[4:6].sum())\n",
    "\n",
    "print(\"\\nClass 2:\" )\n",
    "# print(person_class[person_class.Class == 2].value_counts(\"max_day\", normalize=True).sort_index(ascending=True))\n",
    "# print sum of 5 & 6, percentage\n",
    "print(person_class[person_class.Class == 2].value_counts(\"max_day\", normalize=True).sort_index(ascending=True)[4:6].sum())\n",
    "\n",
    "print(\"\\nClass 3:\" )\n",
    "# print(person_class[person_class.Class == 3].value_counts(\"max_day\", normalize=True).sort_index(ascending=True))\n",
    "# print sum of 5 & 6, percentage\n",
    "print(person_class[person_class.Class == 3].value_counts(\"max_day\", normalize=True).sort_index(ascending=True)[4:6].sum())\n",
    "\n",
    "print(\"\\nClass 4:\" )\n",
    "# print(person_class[person_class.Class == 4].value_counts(\"max_day\", normalize=True).sort_index(ascending=True))\n",
    "# print sum of 5 & 6, percentage\n",
    "print(person_class[person_class.Class == 4].value_counts(\"max_day\", normalize=True).sort_index(ascending=True)[4:6].sum())\n",
    "\n",
    "# print size of each class\n",
    "print(\"\\nClass 1 size: \", person_class[person_class.Class == 1].shape[0])\n",
    "print(\"Class 2 size: \", person_class[person_class.Class == 2].shape[0])\n",
    "print(\"Class 3 size: \", person_class[person_class.Class == 3].shape[0])\n",
    "print(\"Class 4 size: \", person_class[person_class.Class == 4].shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Yelp for restaurant-level information\n",
    "- note: about 4000 entries are missing price, range or category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_1 = pd.read_csv(\"data_outputs/trips_od_yelp_1.csv\")\n",
    "rest_2 = pd.read_csv(\"data_outputs/trips_od_yelp_2.csv\")\n",
    "rest_3 = pd.read_csv(\"data_outputs/trips_od_yelp_3.csv\")\n",
    "rest_4 = pd.read_csv(\"data_outputs/trips_od_yelp_4.csv\")\n",
    "rest_5 = pd.read_csv(\"data_outputs/trips_od_yelp_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_pois = pd.concat([rest_1, rest_2, rest_3, rest_4, rest_5])\n",
    "rest_pois = rest_pois.drop([\"Unnamed: 0.1\", \"Unnamed: 0\"], axis = 1)\n",
    "\n",
    "home_loc = home_loc[[\"household_id\", \"final_home_lat\", \"final_home_lng\"]]\n",
    "rest_pois = pd.merge(rest_pois, home_loc, on=\"household_id\", how=\"left\")\n",
    "\n",
    "rest_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 2021 trips\n",
    "rest_pois = rest_pois[rest_pois[\"survey_year\"] != 2021]\n",
    "rest_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yelp_extractor(row):\n",
    "    try:\n",
    "        yelp_dict = ast.literal_eval(row[\"restaurant_info\"])\n",
    "\n",
    "        name = None\n",
    "        price = None\n",
    "        rating = None\n",
    "        categories = None\n",
    "        # make sure dict has all four required categories\n",
    "        if len(yelp_dict) > 0:\n",
    "            tot_keys = list(yelp_dict.keys())\n",
    "            if \"name\" in tot_keys:\n",
    "                name = yelp_dict[\"name\"]\n",
    "            if \"price\" in tot_keys:\n",
    "                price = yelp_dict[\"price\"]\n",
    "            if \"rating\" in tot_keys:\n",
    "                rating = yelp_dict[\"rating\"]\n",
    "            if \"categories\" in tot_keys:\n",
    "                categories = []\n",
    "                all_cats = yelp_dict[\"categories\"]\n",
    "                for ea_cat in all_cats:\n",
    "                    categories.append(ea_cat[\"title\"])\n",
    "\n",
    "        return pd.Series([name, price, rating, categories])\n",
    "    except:\n",
    "        print(row.name)\n",
    "        return pd.Series([None, None, None, None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_pois.restaurant_info.isna().sum()/rest_pois.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply yelp extractor function to dataframe\n",
    "rest_pois[[\"name\", \"price\", \"rating\", \"categories\"]] = rest_pois.apply(yelp_extractor, axis=1)\n",
    "# drop restaurants who have price, rating, or category of \"None\"\n",
    "rest_pois = rest_pois.dropna(axis=0, subset = ['price', 'rating', 'categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace categories column in df with a single caetgory\n",
    "# look up first value in categories, replace with category simple key\n",
    "def simplify_food_cat(row):\n",
    "    # simplified categories, from chat GPT\n",
    "    category_simple = dict()\n",
    "    category_simple[\"bakery\"] = [\"Kombucha\",'Tea Rooms','Creperies','Chocolatiers & Shops','Coffee Roasteries','Cafes', \"Acai Bowls\", \"Bagels\", \"Bakeries\", \"Bubble Tea\", \"Candy Stores\", \"Coffee & Tea\", \"Cupcakes\", \"Custom Cakes\", \"Desserts\", \"Donuts\", \"Gelato\", \"Ice Cream & Frozen Yogurt\", \"Macarons\", \"Pancakes\", \"Patisserie/Cake Shop\", \"Pretzels\", \"Shaved Ice\", \"Waffles\"]\n",
    "    category_simple[\"bars\"]= [\"Beer Tours\", 'Cideries','Distilleries','Brewpubs','Bars', 'Beer Bar', 'Beer Gardens', 'Beer, Wine & Spirits', 'Breweries','Champagne Bars', 'Cocktail Bars', 'Dive Bars', 'Gay Bars', 'Gastropubs', 'Irish Pub', 'Karaoke', 'Lounges', 'Music Venues', 'Pubs', 'Speakeasies', 'Sports Bars', 'Whiskey Bars', 'Wine Bars', 'Wine Tasting Room', 'Wineries', 'Pool Halls', 'Venues & Event Spaces']\n",
    "    category_simple[\"american\"] = [\"Fast Food\",'Delis','Chicken Wings','Cajun/Creole', 'Barbeque',\"American (New)\", \"American (Traditional)\", \"Burgers\", \"Breakfast & Brunch\", \"Cheesesteaks\", \"Comfort Food\", \"Diners\", \"Hot Dogs\", \"Sandwiches\", \"Soul Food\", \"Southern\", \"Steakhouses\", \"Hot Dogs\", \"New Mexican Cuisine\", \"Salad\", \"Wraps\"]\n",
    "    category_simple[\"asian\"] = [\"Sri Lankan\",'Teppanyaki','Hawaiian','Poke', \"Asian Fusion\", \"Cantonese\", \"Chinese\", \"Conveyor Belt Sushi\", \"Dim Sum\", \"Izakaya\", \"Japanese\", \"Japanese Curry\", \"Korean\", \"Laotian\", \"Malaysian\", \"Mongolian\", \"Noodles\", \"Pakistani\", \"Pan Asian\", \"Ramen\", \"Shanghainese\", \"Sushi Bars\", \"Taiwanese\", \"Thai\", \"Vietnamese\", \"Cambodian\", \"Indian\", \"Filipino\", \"Himalayan/Nepalese\", \"Szechuan\", \"Hot Pot\"]\n",
    "    category_simple[\"european\"] = ['Sicilian','Fish & Chips',\"British\", \"French\", \"German\", \"Italian\", \"Modern European\", \"Polish\", \"Russian\", \"Sardinian\", \"Scandinavian\", \"Scottish\", \"Spanish\", \"Tapas/Small Plates\", \"Turkish\", \"Tuscan\", \"Tapas Bars\", \"Irish\", \"Australian\", \"Pizza\"]\n",
    "    category_simple[\"hispanic\"] = [\"Argentine\",'Tex-Mex',\"Brazilian\", \"Cuban\", \"Empanadas\", \"Honduran\", \"Latin American\", \"Mexican\", \"Nicaraguan\", \"Peruvian\", \"Salvadoran\", \"Tacos\", \"Venezuelan\", \"Trinidadian\"]\n",
    "    category_simple[\"mediterranean\"] = ['Halal','Falafel',\"Greek\", \"Lebanese\", \"Mediterranean\", \"Middle Eastern\", \"Persian/Iranian\", \"Syrian\", \"Afghan\"]\n",
    "    category_simple[\"african\"] = ['Moroccan',\"African\", \"Egyptian\", \"Eritrean\", \"Ethiopian\", \"Somali\", 'Caribbean']\n",
    "    category_simple[\"specialty\"] = [\"Pop-Up Restaurants\",\"Dinner Theater\",'Honey','Chicken Shop','Caterers','Themed Cafes','Soup','Vegetarian','Butcher','Vegan','Juice Bars & Smoothies', 'Buffets','Cafeteria','Cheese Shops', 'Farmers Market', 'Fruits & Veggies', 'Gluten-Free', 'Health Markets', 'Imported Food', 'International Grocery', 'Meat Shops', 'Nutritionists', 'Organic Stores', 'Seafood Markets', 'Specialty Food', 'Vitamins & Supplements', 'Food Court', 'Food Delivery Services', 'Food Stands', 'Food Trucks', 'Herbal Shops', 'Herbs & Spices', 'Seafood', 'Street Vendors', 'Traditional Chinese Medicine', 'Imported Food', 'Local Flavor', 'Eatertainment', 'Food Tours']\n",
    "    category_simple[\"misc\"] = [\"Casinos\",\"Saunas\",\"Yoga\",\"Pop-up Shops\",\"Furniture Stores\",\"Restaurants\",\"Hair Salons\",\"CSA\",'Pharmacy','Food','Grocery','Accessories', 'Arcades', 'Art Galleries', 'Art Museums', 'Auto Repair', 'Bikes', 'Boat Charters', 'Boating', 'Bocce Ball', 'Bookstores', 'Car Wash', 'Convenience Stores', 'Cooking Classes', 'Cooking Schools', 'Cosmetics & Beauty Supply', 'Department Stores', 'Drugstores', 'Florists', 'Gas Stations', 'Gift Shops', 'Hobby Shops', 'Home Decor', 'Mini Golf', 'Mobile Phones', 'Nurseries & Gardening', 'Photography Stores & Services', 'Propane', 'Sports Wear', 'Tobacco Shops', 'Toy Stores', 'Trainers', 'Vinyl Records']\n",
    "    \n",
    "    fast_food = False\n",
    "    simple_cat = None\n",
    "    if row.categories and len(row.categories) > 0:\n",
    "        if \"Fast Food\" in row.categories:\n",
    "            fast_food = True\n",
    "            \n",
    "        cat = row.categories[0]\n",
    "        for key, ls in category_simple.items():\n",
    "            if cat in ls:\n",
    "                simple_cat = key\n",
    "                break\n",
    "    else:\n",
    "        print('NoneType')\n",
    "        \n",
    "    return pd.Series([fast_food, simple_cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify and quantify food categories, count number of total transactions (including dine-in)\n",
    "# count price, round ratings to nearest int,\n",
    "rest_pois[[\"fast_food\",\"simple_cat\"]] = rest_pois.apply(simplify_food_cat, axis=1)\n",
    "rest_pois['int_fast_food'] = pd.factorize(rest_pois[\"fast_food\"], sort=True)[0] + 1\n",
    "rest_pois['int_simple_cat'] = pd.factorize(rest_pois[\"simple_cat\"], sort=True)[0] + 1 \n",
    "rest_pois[\"int_price\"] = rest_pois[\"price\"].apply(lambda x: len(x))\n",
    "rest_pois[\"int_rating\"] = rest_pois[\"rating\"].apply(lambda x: int(np.around(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding BE info for restaurant-level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BE info for restaurants (o_tract10, d_tract10)\n",
    "employ_pop = # download ACS data with # employed individuals per census tract\n",
    "income = # download ACS data with median household income per census tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up income columns, extract census tract numbers, drop rows with \"-\"\n",
    "income_clean = income[[\"GEO_ID\", \"B19013_001E\"]].tail(-1).reset_index(drop=True)\n",
    "income_clean[\"census_tract\"] = income_clean[\"GEO_ID\"].apply(lambda x: x[9:])\n",
    "income_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up employment population and convert tract to FIPS id\n",
    "employ_pop = employ_pop[[\"data_year\", \"County\", \"Tract\", \"Total\"]]\n",
    "employ_pop = employ_pop[employ_pop.County != \"Region\"]\n",
    "employ_pop = employ_pop[~np.isnan(employ_pop.Tract)]\n",
    "\n",
    "# function to convert census tract numbers to FIPS codes\n",
    "def tract_to_fips(row):\n",
    "    if np.isnan(row.Tract):\n",
    "        return\n",
    "    # Define a dictionary of county FIPS codes for Washington\n",
    "    all_county_fips = {'King': '033', 'Pierce': '053', 'Snohomish': '061', 'Kitsap': '035'}\n",
    "    county_fips = all_county_fips[row.County]\n",
    "    # Define the state FIPS code for Washington\n",
    "    state_fips = '53'\n",
    "    \n",
    "    census_tract = str(int(float(row.Tract) * 100))\n",
    "    fips_code = str(state_fips) + str(county_fips).zfill(3) + census_tract.zfill(6)\n",
    "    \n",
    "    return fips_code\n",
    "\n",
    "employ_pop[\"tract_FIPS\"] = employ_pop.apply(tract_to_fips, axis=1)\n",
    "employ_pop[\"Total\"] = employ_pop.Total.apply(lambda x: int(x) if x != \"suppressed\" else None)\n",
    "employ_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employ_pop = employ_pop[~np.isnan(employ_pop.Total)]\n",
    "employ_pop_group = employ_pop.groupby(\"tract_FIPS\").Total.median().reset_index().sort_values(by=\"Total\")\n",
    "employ_pop_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any rows with nans in origin or destination tracts\n",
    "rest_pois = rest_pois.dropna(axis=0, subset = ['o_tract10', 'd_tract10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed = 0 \n",
    "def rest_be(row):\n",
    "    global missed\n",
    "    # determine if restaurant is origin or destination, find census tract\n",
    "    if row.origin_purpose == \"Went to restaurant to eat/get take-out\":\n",
    "        rest_census_tr = row.o_tract10\n",
    "    elif row.dest_purpose == \"Went to restaurant to eat/get take-out\":\n",
    "        rest_census_tr = row.d_tract10\n",
    "    \n",
    "    rest_census_tr = str(int(rest_census_tr))\n",
    "\n",
    "    # look up census tract in pop and income dfs\n",
    "    rest_empl_pop = None\n",
    "    try:\n",
    "        rest_empl_pop = employ_pop_group[employ_pop_group.tract_FIPS == rest_census_tr][\"Total\"].values[0]\n",
    "    except IndexError:\n",
    "        print(\"Not in index\")\n",
    "        missed += 1\n",
    "    rest_inc = income_clean[income_clean.census_tract == rest_census_tr][\"B19013_001E\"].values[0]\n",
    "    return pd.Series([rest_empl_pop, rest_inc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rest_pois[[\"rest_empl_pop\", \"rest_inc\"]] = rest_pois.apply(rest_be, axis=1)\n",
    "print(missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of empl_pop == None (index didn't match up)\n",
    "rest_pois = rest_pois[~rest_pois.rest_empl_pop.isna()]\n",
    "rest_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert income to int datatype\n",
    "rest_pois.loc[rest_pois[\"rest_inc\"]=='-', \"rest_inc\"] = np.nan\n",
    "rest_pois.loc[rest_pois[\"rest_inc\"]=='250,000+', \"rest_inc\"] = 250000\n",
    "rest_pois[\"rest_inc\"] = rest_pois[\"rest_inc\"].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience-- distance from home\n",
    "def dist_latlng(row):\n",
    "    if row.origin_purpose == \"Went to restaurant to eat/get take-out\":\n",
    "        coords_1 = (row.origin_lat, row.origin_lng)\n",
    "    elif row.dest_purpose == \"Went to restaurant to eat/get take-out\":\n",
    "        coords_1 = (row.dest_lat, row.dest_lng)\n",
    "        \n",
    "    coords_2 = (row.final_home_lat, row.final_home_lng)\n",
    "    \n",
    "    return geopy.distance.geodesic(coords_1, coords_2).miles\n",
    "\n",
    "rest_pois[\"dist_from_home\"] = rest_pois.apply(dist_latlng, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine restaurant with non-restaurant trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID non-restaurant trips\n",
    "trips_od_nonrest = trips_od[(trips_od[\"origin_purpose\"]!=\"Went to restaurant to eat/get take-out\")\n",
    "                            & (trips_od[\"dest_purpose\"]!=\"Went to restaurant to eat/get take-out\")]\n",
    "trips_od_nonrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine restaurant and non-restaurant trips\n",
    "trips_w_pois = pd.concat([rest_pois, trips_od_nonrest])\n",
    "trips_w_pois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding BE data for the person level\n",
    "- restaurant in food desert, home in food desert?\n",
    "- note: FARA (food desert) information is based off 2010 census tracts, and was updated in 2019. this data already has 2010 population and median family income information.\n",
    "- Note: a food desert by USDA is defined as living more than 1 mile (in urban) or 20 miles (in rural) areas from a grocery store, and low income (LILA at 1 and 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 2021 data\n",
    "household = household[household[\"survey_year\"] != 2021]\n",
    "person = person[person[\"survey_year\"] != 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in USDA Food Access data\n",
    "fara = # download USDA Food Access Research Atlas - 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select census tracts located within PSRC\n",
    "psrc_fara = fara[(fara.State == \"Washington\") & ((fara.County == \"King County\") |\n",
    "                                  (fara.County == \"Pierce County\") | \n",
    "                                   (fara.County == \"Snohomish County\") |\n",
    "                                  (fara.County == \"Kitsap County\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to join household to person data to obtain home tract\n",
    "hh_info = household[[\"sample_county\", \"final_home_tract10\", \"household_id\"]]\n",
    "person = person.merge(hh_info, on = \"household_id\", how=\"left\")\n",
    "# only look at people who made restaurant trips\n",
    "person_rest = person[person.person_id.isin(rest_person_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a person lives in a home located in a food desert _-> home_fd = True, else = False\n",
    "def in_fd(row):\n",
    "    # tract of home\n",
    "    census_tract = row.final_home_tract10 \n",
    "    if psrc_fara[psrc_fara.CensusTract == census_tract][\"LILATracts_1And10\"].values[0] == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "person_rest[\"home_fd\"] = person_rest.apply(in_fd, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate trips to the person level\n",
    "- `trips_w_pois` is df with all trips made by people who made restaurant trips in the dataset\n",
    "-`person_rest` is df with unique people who made restaurant trips\n",
    "- characteristics from `trips_w_pois` should be aggregated for each unique person and added onto the `person_rest` df\n",
    "    - travel time: median\n",
    "    - mode: most common mode\n",
    "    - trip purposes: most common trip purpose\n",
    "    - time of day: most common time of day\n",
    "    - day of week: most common day of week\n",
    "    - ratio of restaurant trips to all trips: (not aggreggated)\n",
    "    - food categories: most common, number of food categories visited\n",
    "    - fast food: yes if visited in any of the trips, % fast food visits out of restaurant visits\n",
    "    - distance from home: median\n",
    "    - price: median\n",
    "    - rating: median\n",
    "    - population: median\n",
    "    - income: median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting time of day to mealtime\n",
    "def time_meal(date_ls):\n",
    "    meal_ls = []\n",
    "    for date in date_ls:\n",
    "        date_string = date[:-1]\n",
    "        date_format = \"Date: %Y-%m-%d %H:%M:%S.%f\"\n",
    "        five_am = datetime.strptime(\"05:00\", \"%H:%M\").time()\n",
    "        ten_am = datetime.strptime(\"10:00\", \"%H:%M\").time()\n",
    "        two_pm = datetime.strptime(\"14:00\", \"%H:%M\").time()\n",
    "        five_pm = datetime.strptime(\"17:00\", \"%H:%M\").time()\n",
    "        nine_pm = datetime.strptime(\"21:00\", \"%H:%M\").time()\n",
    "        tod = datetime.strptime(date_string, date_format).time()\n",
    "        if tod > five_am and tod < ten_am:\n",
    "            meal_ls.append(\"Breakfast\")\n",
    "        elif tod > ten_am and tod < two_pm:\n",
    "            meal_ls.append(\"Lunch\")\n",
    "        elif tod > five_pm and tod < nine_pm:\n",
    "            meal_ls.append(\"Dinner\")\n",
    "        else:\n",
    "            meal_ls.append(\"Other\")\n",
    "    return max(set(meal_ls), key=meal_ls.count)\n",
    "\n",
    "meal_pref = rest_pois.groupby(\"person_dim_id\")[\"depart_time_string\"].apply(list).reset_index()\n",
    "meal_pref[\"meal\"] = meal_pref.depart_time_string.apply(time_meal)\n",
    "meal_pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: THIS SHOULD ONLY BE RUN ON RESTAURANT TRIPS\n",
    "# travel time\n",
    "med_travel_time = rest_pois.groupby(\"person_dim_id\")[[\"travel_time_calc\"]].median()\n",
    "med_travel_time = med_travel_time.rename({\"travel_time_calc\":\"med_trav_time\"}, axis=1)\n",
    "\n",
    "# mode (note: don't want nan)\n",
    "all_modes = rest_pois.groupby(\"person_dim_id\")[\"main_mode\"].apply(list).reset_index()\n",
    "all_modes[\"pref_modes\"] = all_modes[\"main_mode\"].apply(lambda x:Counter(x).most_common(1)[0][0])\n",
    "\n",
    "# trip ratios\n",
    "rest_trips = rest_pois.groupby(\"person_dim_id\").trip_id.count().reset_index().rename({\"trip_id\":\"rest_trips\"}, axis=1)\n",
    "non_rest_trips = trips_od_nonrest.groupby(\"person_dim_id\").trip_id.count().reset_index().rename({\"trip_id\":\"nonrest_trips\"}, axis=1)\n",
    "trip_ratios = pd.merge(rest_trips, non_rest_trips, on=\"person_dim_id\", how=\"left\")\n",
    "trip_ratios[\"rest:non_rest\"] = trip_ratios[\"rest_trips\"]/trip_ratios[\"nonrest_trips\"]\n",
    "\n",
    "# food categories\n",
    "all_food_cat = rest_pois.groupby(\"person_dim_id\")[\"simple_cat\"].apply(list).reset_index()\n",
    "all_food_cat[\"diversity_food_cat\"] = all_food_cat[\"simple_cat\"].apply(lambda x: len(set(x)))\n",
    "all_food_cat[\"pref_food_cat\"] = all_food_cat[\"simple_cat\"].apply(lambda x: Counter(x).most_common(1)[0][0])\n",
    "\n",
    "# fast food\n",
    "fast_food = rest_pois.groupby(\"person_dim_id\")[\"fast_food\"].sum().reset_index()\n",
    "fast_food = pd.merge(rest_trips, fast_food, on=\"person_dim_id\")\n",
    "fast_food[\"fast_food_flag\"] = fast_food[\"fast_food\"].apply(lambda x: True if x>0 else False)\n",
    "fast_food[\"pct_fast_food\"] = fast_food[\"fast_food\"]/fast_food[\"rest_trips\"]\n",
    "\n",
    "# price\n",
    "price_avg = rest_pois.groupby(\"person_dim_id\")[[\"int_price\"]].median()\n",
    "price_avg = price_avg.rename({\"int_price\":\"med_price\"}, axis=1)\n",
    "\n",
    "# rating\n",
    "rating_avg = rest_pois.groupby(\"person_dim_id\")[[\"rating\"]].median()\n",
    "rating_avg = rating_avg.rename({\"rating\":\"med_rating\"}, axis=1)\n",
    "\n",
    "#population\n",
    "rest_empl_avg = rest_pois.groupby(\"person_dim_id\")[[\"rest_empl_pop\"]].median()\n",
    "rest_empl_avg = rest_empl_avg.rename({\"Total\":\"med_rest_empl\"}, axis=1)\n",
    "\n",
    "# income\n",
    "rest_inc_avg = rest_pois.groupby(\"person_dim_id\")[[\"rest_inc\"]].median()\n",
    "rest_inc_avg = rest_inc_avg.rename({\"rest_inc\":\"med_rest_inc\"}, axis=1)\n",
    "\n",
    "# distance from home\n",
    "dist_home_avg = rest_pois.groupby(\"person_dim_id\")[[\"dist_from_home\"]].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trip purpose (note: don't want missing)\n",
    "def nonrest_purp(row):\n",
    "    purp = None\n",
    "    if row.origin_purpose_cat == \"Meal\":\n",
    "        purp = row.dest_purpose_cat\n",
    "    elif row.dest_purpose_cat == \"Meal\":\n",
    "        purp = row.origin_purpose_cat\n",
    "    return purp\n",
    "\n",
    "rest_pois[\"nonrest_purp\"] = rest_pois.apply(nonrest_purp, axis=1)\n",
    "\n",
    "trip_purp = rest_pois.groupby(\"person_dim_id\")[\"nonrest_purp\"].apply(list).reset_index()\n",
    "trip_purp[\"nonrest_purp\"] = trip_purp[\"nonrest_purp\"].apply(lambda x: Counter(x).most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_agg = pd.merge(person_rest, med_travel_time, right_on=\"person_dim_id\", left_on=\"person_id\", how='left')\n",
    "person_agg = pd.merge(person_agg, all_modes[[\"person_dim_id\",\"pref_modes\"]], left_on = \"person_id\", right_on = \"person_dim_id\", how='left')\n",
    "person_agg = person_agg.drop([\"person_dim_id\"], axis=1)\n",
    "person_agg = pd.merge(person_agg, all_food_cat[[\"person_dim_id\",\"diversity_food_cat\"]], left_on = \"person_id\", right_on = \"person_dim_id\", how='left')\n",
    "person_agg = person_agg.drop([\"person_dim_id\"], axis=1)\n",
    "person_agg = pd.merge(person_agg, fast_food[[\"person_dim_id\", \"pct_fast_food\"]], left_on = \"person_id\", right_on = \"person_dim_id\", how='left')\n",
    "person_agg = person_agg.drop([\"person_dim_id\"], axis=1)\n",
    "person_agg = pd.merge(person_agg, price_avg, right_on=\"person_dim_id\", left_on=\"person_id\", how='left')\n",
    "person_agg = pd.merge(person_agg, rating_avg, right_on=\"person_dim_id\", left_on=\"person_id\", how='left')\n",
    "person_agg = pd.merge(person_agg, rest_empl_avg, right_on=\"person_dim_id\", left_on=\"person_id\", how='left')\n",
    "person_agg = pd.merge(person_agg, rest_inc_avg, right_on=\"person_dim_id\", left_on=\"person_id\", how='left')\n",
    "person_agg = pd.merge(person_agg, dist_home_avg, right_on=\"person_dim_id\", left_on=\"person_id\", how='left')\n",
    "person_agg = pd.merge(person_agg, trip_ratios[[\"person_dim_id\",\"rest:non_rest\"]], right_on=\"person_dim_id\", left_on=\"person_id\", how='left')\n",
    "person_agg = person_agg.drop([\"person_dim_id\"], axis=1)\n",
    "person_agg = pd.merge(person_agg, trip_purp, left_on = \"person_id\", right_on = \"person_dim_id\", how='left')\n",
    "person_agg = person_agg.drop([\"person_dim_id\"], axis=1)\n",
    "person_agg = pd.merge(person_agg, meal_pref[[\"person_dim_id\",\"meal\"]], left_on = \"person_id\", right_on = \"person_dim_id\", how='left')\n",
    "person_agg = person_agg.drop([\"person_dim_id\"], axis=1)\n",
    "person_agg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up data for analysis\n",
    "- drop columns\n",
    "- transform categorical --> numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in person_agg.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_keep = [\"household_id\", \"person_id\", \"survey_year\", \"seattle_home\",\n",
    "               \"sample_county\", \"hhsize\", \"vehicle_count\", \"hhincome_broad\", \"hhincome_detailed\", \n",
    "               \"rent_own\", \"numchildren\", \"age\", \"gender\", \"employment\", \"student\", \"education\",\n",
    "               \"license\", \"race_category\", \"hh_race_category\", \"commute_freq\", \n",
    "                \"prev_res_factors_housing_cost\",\n",
    "                \"prev_res_factors_income_change\", \"prev_res_factors_community_chan\",\n",
    "                \"prev_res_factors_hh_size\", \"prev_res_factors_more_space\",\n",
    "                \"prev_res_factors_less_space\", \"prev_res_factors_employment\",\n",
    "                \"prev_res_factors_school\", \"prev_res_factors_crime\", \"prev_res_factors_quality\",\n",
    "                \"prev_res_factors_forced\", \"prev_res_factors_no_answer\", \"prev_res_factors_other\",\n",
    "                \"prev_res_factors_specify\", \"res_factors_30min\", \"res_factors_afford\",\n",
    "                \"res_factors_closefam\", \"res_factors_hhchange\", \"res_factors_hwy\",\n",
    "                \"res_factors_school\", \"res_factors_space\", \"res_factors_transit\",\n",
    "                \"res_factors_walk\", \"home_fd\", \"med_trav_time\", \"pref_modes\", \"diversity_food_cat\",\n",
    "            \"pct_fast_food\", \"med_price\", \"med_rating\", \"rest_empl_pop\", \"med_rest_inc\",\n",
    "            \"dist_from_home\",\"rest:non_rest\", \"nonrest_purp\", \"meal\"]\n",
    "final_person_data = person_agg[vars_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "final_person_data.to_csv(\"data_outputs/final_person_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trajectory_clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
